{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "Takes about 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture installation_log\n",
    "\n",
    "import sys\n",
    "\n",
    "# Install Miniconda\n",
    "!wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "!bash Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
    "!rm Miniconda3-latest-Linux-x86_64.sh\n",
    "\n",
    "# Install RDKit\n",
    "!conda install -y -c conda-forge rdkit\n",
    "\n",
    "# Install PyPI packages\n",
    "!pip install wget\n",
    "\n",
    "# Clone smiles repository\n",
    "!git clone https://github.com/mrezler/smiles.git\n",
    "\n",
    "# Extend `sys.path`\n",
    "sys.path.append('/usr/local/lib/python3.7/site-packages/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory to `/content/smiles`\n",
    "cd smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer import datasets, serializers, training, Variable\n",
    "from chainer.training import extensions\n",
    "from feature import *\n",
    "from os import makedirs\n",
    "from rdkit import Chem\n",
    "from sklearn import metrics\n",
    "from time import time\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import SCFPfunctions as Mf\n",
    "import SCFPmodel as Mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATOM_INFO = 21       # Size of atom feature vector. Default = 21\n",
    "ATOM_SIZE = 400      # Max length of smiles. Default = 400\n",
    "BATCH_SIZE = 32      # Number of moleculars in each mini-batch. Default = 32\n",
    "BOOST = -1           # Augmentation rate (-1 indicates OFF). Default = -1\n",
    "DATA_DIR = 'TOX21'   # Input Smiles Dataset. Default = 'TOX21'\n",
    "EPOCH = 500          # Number of max iteration to evaluate. Default = 500\n",
    "F1 = 128             # No. of filters of first convolution layer. Default = 128\n",
    "F3 = 64              # No. of filters of second convolution layer. Default = 64\n",
    "FREQUENCY = 1        # Epoch frequency for evaluation. Default = 1\n",
    "GPU = 0              # GPU ID (negative value indicates CPU). Default = -1\n",
    "K1 = 11              # Window-size of first convolution layer. Default = 11\n",
    "K2 = 5               # Window-size of first pooling layer. Default = 5\n",
    "K3 = 11              # Window-size of second convolution layer. Default = 11\n",
    "K4 = 5               # Window-size of second pooling layer. Default = 5\n",
    "MODEL_DIR = 'MODEL'  # Directory to Model to evaluate. Default = 'OUT'\n",
    "N_HID = 96           # No. of hidden perceptron. Default = 96\n",
    "N_OUT = 1            # No. of output perceptron (class). Default = 1\n",
    "PROTEIN = 'NR-AR'    # Name of protein (subdataset). Default = 'NR-AR'\n",
    "S1 = 1               # Stride-step of first convolution layer. Default = 1\n",
    "S2 = 1               # Stride-step of first max-pooling layer. Default = 1\n",
    "S3 = 1               # Stride-step of second convolution layer. Default = 1\n",
    "S4 = 1               # Stride-step of second pooling layer. Default = 1\n",
    "SCORE_SX = 'score'   # Suffix of final scoring data files. Default = 'score'\n",
    "STRUCT_INFO = 21     # Size of structural feature vector. Default = 21\n",
    "TEST_SX = 'test'     # Suffix test data files. Default = 'test'\n",
    "TRAIN_SX = \\\n",
    "    'wholetraining'  # Suffix training data files. Default = 'wholetraining'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lensize = ATOM_INFO + STRUCT_INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(name, suffix, sep):\n",
    "    print(f'Making {name} dataset...')\n",
    "    file = DATA_DIR + '/' + PROTEIN + '_' + suffix + '.smiles'\n",
    "    print(f'Loading smiles: {file}')\n",
    "    smi = Chem.SmilesMolSupplier(file, delimiter=sep, titleLine=False)\n",
    "    mols = [mol for mol in smi if mol is not None]\n",
    "    F_list, T_list = [], []\n",
    "    for mol in mols:\n",
    "        smiles = Chem.MolToSmiles(mol, kekuleSmiles=True, isomericSmiles=True)\n",
    "        if len(smiles) > ATOM_SIZE:\n",
    "            print('WARNING: Too long mol was ignored.')\n",
    "        else:\n",
    "            F_list.append(mol_to_feature(mol, -1, ATOM_SIZE))\n",
    "            T_list.append(mol.GetProp('_Name'))\n",
    "    Mf.random_list(F_list)\n",
    "    Mf.random_list(T_list)\n",
    "    data_t = np.asarray(T_list, dtype=np.int32).reshape(-1, N_OUT)\n",
    "    data_f = np.asarray(F_list, dtype=np.float32).reshape(-1, N_OUT, ATOM_SIZE,\n",
    "                                                          lensize)\n",
    "    return data_f, data_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `trainer-challenge.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training settings at start\n",
    "settings = f\"\"\"\n",
    "TRAINING SETTINGS:\n",
    "\n",
    "GPU ID: {GPU}\n",
    "Minibatch-size: {BATCH_SIZE}\n",
    "Epoch: {EPOCH}\n",
    "\\t\\t\\tWindow-size\\tStride-step\\tNo. of filters\n",
    "1st convolution:\\t{K1}\\t\\t{S1}\\t\\t{F1}\n",
    "Max-pooling:\\t\\t{K2}\\t\\t{S2}\\t\\t---\n",
    "2nd convolution:\\t{K3}\\t\\t{S3}\\t\\t{F3}\n",
    "Max-pooling:\\t\\t{K4}\\t\\t{S4}\\t\\t---\n",
    "\"\"\"\n",
    "print(settings)\n",
    "# Turn timer on\n",
    "start = time()\n",
    "# Choose CPU or GPU\n",
    "xp = np\n",
    "if GPU >= 0:\n",
    "    print('GPU mode')\n",
    "    xp = cp\n",
    "# Load training dataset\n",
    "X_train, y_train = make_dataset('training', TRAIN_SX, sep=' ')\n",
    "train_dataset = datasets.TupleDataset(X_train, y_train)\n",
    "# Load testing dataset\n",
    "X_test, y_test = make_dataset('testing', TEST_SX, sep='\\t')\n",
    "test_dataset = datasets.TupleDataset(X_test, y_test)\n",
    "# Define model\n",
    "model = Mm.CNN(ATOM_SIZE, lensize, K1, S1, F1, K2, S2, K3, S3, F3, K4, S4,\n",
    "               N_HID, N_OUT)\n",
    "# Transfer model to GPU if GPU mode is set\n",
    "if GPU >= 0:\n",
    "    chainer.cuda.get_device_from_id(GPU).use()\n",
    "    model.to_gpu()\n",
    "# Define optimizer\n",
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(model)\n",
    "# Trainer settings\n",
    "print('Trainer is setting up...')\n",
    "output_dir = MODEL_DIR + '/' + PROTEIN\n",
    "makedirs(output_dir)\n",
    "train_iter = chainer.iterators.SerialIterator(train_dataset,\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              shuffle=True)\n",
    "test_iter = chainer.iterators.SerialIterator(test_dataset,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             repeat=False, shuffle=True)\n",
    "updater = training.StandardUpdater(train_iter, optimizer, device=GPU)\n",
    "trainer = training.Trainer(updater, (EPOCH, 'epoch'), out=output_dir)\n",
    "trainer.extend(extensions.Evaluator(test_iter, model, device=GPU))\n",
    "trainer.extend(extensions.snapshot_object(model,\n",
    "                                          'model_snapshot_{.updater.epoch}'),\n",
    "               trigger=(FREQUENCY, 'epoch'))\n",
    "trainer.extend(extensions.LogReport(trigger=(1, 'epoch'),\n",
    "                                    log_name='log_epoch'))\n",
    "trainer.extend(extensions.LogReport(trigger=(10, 'iteration'),\n",
    "                                    log_name='log_iteration'))\n",
    "trainer.extend(extensions.PrintReport(['epoch', 'elapsed_time', 'main/loss',\n",
    "                                       'validation/main/loss', 'main/accuracy',\n",
    "                                       'validation/main/accuracy']))\n",
    "# Run trainer\n",
    "print('Trainer is running...')\n",
    "trainer.run()\n",
    "# Turn timer off and print summary of training\n",
    "end = time()\n",
    "print(f'Training is done. Total time: {int((end-start)//60)} minutesï¼Ž')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `evaluate-challenge.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "X_score, y_score = make_dataset('scoring', SCORE_SX, sep='\\t')\n",
    "# Evaluator settings\n",
    "borders = [len(y_score) * i // 30 for i in range(30+1)]\n",
    "with cp.cuda.Device(GPU):\n",
    "    X_score_gpu = cp.array(X_score)\n",
    "    y_score_gpu = cp.array(y_score)\n",
    "# Load model\n",
    "model = Mm.CNN(ATOMSIZE, lensize, K1, S1, F1, K2, S2, K3, S3, F3, K4, S4,\n",
    "               N_HID, N_OUT)\n",
    "model.compute_accuracy = False\n",
    "model.to_gpu(GPU)\n",
    "# Run evaluator\n",
    "print('Evaluator is  running...')\n",
    "f = open(MODEL_DIR + '/' + PROTEIN + '/evaluation_epoch.csv', 'w')\n",
    "print('epoch', 'TP', 'FN', 'FP', 'TN', 'Loss', 'Accuracy', 'B_accuracy',\n",
    "      'Sepecificity', 'Precision', 'Recall', 'F-measure', 'AUC', sep='\\t')\n",
    "f.write('epoch,TP,FN,FP,TN,Loss,Accuracy,B_accuracy,Sepecificity,Precision,'\n",
    "        'Recall,F-measure,AUC\\n')\n",
    "for epoch in range(FREQUENCY, EPOCH+1, FREQUENCY):\n",
    "    pred_score, loss = [], []\n",
    "    with cp.cuda.Device(GPU):\n",
    "        npz = MODEL_DIR + '/' + PROTEIN + '/model_snapshot_' + str(epoch)\n",
    "        serializers.load_npz(npz, model)\n",
    "    for i in range(30):\n",
    "        with cp.cuda.Device(GPU):\n",
    "            x_gpu = X_score_gpu[borders[i]:borders[i+1]]\n",
    "            y_gpu = y_score_gpu[borders[i]:borders[i+1]]\n",
    "            pred_tmp_gpu, sr = model.predict(Variable(x_gpu))\n",
    "            pred_tmp_gpu = F.sigmoid(pred_tmp_gpu)\n",
    "            pred_tmp = pred_tmp_gpu.data.get()\n",
    "            loss_tmp = model(Variable(x_gpu), Variable(y_gpu)).data.get()\n",
    "        pred_score.extend(pred_tmp.reshape(-1).tolist())\n",
    "        loss.append(loss_tmp.tolist())\n",
    "    loss = np.mean(loss)\n",
    "    pred_score = np.array(pred_score).reshape(-1, 1)\n",
    "    pred = 1*(pred_score >= 0.5)\n",
    "    count_TP = np.sum(np.logical_and(y_score == pred, pred == 1)*1)\n",
    "    count_FP = np.sum(np.logical_and(y_score != pred, pred == 1)*1)\n",
    "    count_FN = np.sum(np.logical_and(y_score != pred, pred == 0)*1)\n",
    "    count_TN = np.sum(np.logical_and(y_score == pred, pred == 0)*1)\n",
    "    Accuracy = (count_TP+count_TN)/(count_TP+count_FP+count_FN+count_TN)\n",
    "    Sepecificity = count_TN/(count_TN+count_FP)\n",
    "    Precision = count_TP/(count_TP+count_FP)\n",
    "    Recall = count_TP/(count_TP+count_FN)\n",
    "    Fmeasure = 2*Recall*Precision/(Recall+Precision)\n",
    "    B_accuracy = (Sepecificity+Recall)/2\n",
    "    AUC = metrics.roc_auc_score(y_score, pred_score, average='weighted')\n",
    "    params = (epoch, count_TP, count_FN, count_FP, count_TN, loss, Accuracy,\n",
    "              B_accuracy, Sepecificity, Precision, Recall, Fmeasure, AUC)\n",
    "    print(*params, sep='\\t')\n",
    "    text = '{},{},{},{},{},{},{},{},{},{},{},{},{}\\n'.format(*params)\n",
    "    f.write(text)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
